{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c35b3720a18c5c41",
   "metadata": {},
   "source": [
    "# High-Quality Data Augmentation for Low-Resource NMT: Combining Translation Memory, a GAN Generator, and Filtering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26abf4c5-af32-4eb8-918a-08071874dfb6",
   "metadata": {},
   "source": [
    "#### As this is a proof-of-concept demonstration:\n",
    "1. We employ toy data instead of real datasets.\n",
    "\n",
    "2. The translation outputs are primarily intended to illustrate the viability of our approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a73e3720-7418-4b35-a6bf-24303f899200",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a189dee4-7088-4d89-9032-43db22e51238",
   "metadata": {},
   "source": [
    "## Integrating Translation Memory into Input: \n",
    "Translation Memory (TM) is an effective approach to enhance machine translation by providing additional training data for the model.\n",
    "\n",
    "Consequently, we integrated TM into the model's input.\n",
    "\n",
    "<img src=\"./figure/TM.png\" width=\"40%\" style=\"margin: 0 auto;\">\n",
    "\n",
    "- $s$: A German sentence on source side.\n",
    "- $s_{t}$: A German sentence on source side which is similar to $s$.\n",
    "- $t_{t}$: A Upper Sorbian senctence on target side corresponding to $s_{t}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d36f3d6c949f527c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-06T13:34:51.136989Z",
     "start_time": "2024-06-06T13:27:56.952076Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "from retrieval.similar_sentences_selection import read_data, generate_new, similar_domain_selection\n",
    "\n",
    "experiments= \"GAN_TM\"\n",
    "# original bilingual corpus training: 50 * 2, validation: 5 * 2, test: 5 * 2\n",
    "de = './data/toy_data/train.hsb-de.de'\n",
    "hsb = './data/toy_data/train.hsb-de.hsb'\n",
    "val_de = './data/toy_data/devel.hsb-de.de'\n",
    "val_hsb = './data/toy_data/devel.hsb-de.hsb'\n",
    "test_de = './data/toy_data/devel_test.hsb-de.de'\n",
    "test_hsb = './data/toy_data/devel_test.hsb-de.hsb'\n",
    "# target path to write new sentence pair integrated with TM.\n",
    "train_src = './data/{}/train.dehsb_hsb.dehsb'.format(experiments)\n",
    "train_tgt = './data/{}/train.dehsb_hsb.hsb'.format(experiments)\n",
    "val_src = './data/{}/val.dehsb_hsb.dehsb'.format(experiments)\n",
    "val_tgt = './data/{}/val.dehsb_hsb.hsb'.format(experiments)\n",
    "test_src = './data/{}/test.dehsb_hsb.dehsb'.format(experiments)\n",
    "test_tgt = './data/{}/test.dehsb_hsb.hsb'.format(experiments)\n",
    "# separations\n",
    "S = \"[SEP]\"\n",
    "St = \"[SEP]\"\n",
    "Tt = \"[SEP]\"\n",
    "\n",
    "de_lines = read_data(de)\n",
    "hsb_lines = read_data(hsb)\n",
    "val_de_lines = read_data(val_de)\n",
    "val_hsb_lines = read_data(val_hsb)\n",
    "test_de_lines = read_data(test_de)\n",
    "test_hsb_lines = read_data(test_hsb)\n",
    "\n",
    "generate_new(top_k_similar=10, distance=0.5, retrieve_dataset=de_lines, query_lines=de_lines, retrieve_tar_sen_dataset=hsb_lines, query_tar_sen_dataset=hsb_lines, new_src_path=train_src, new_tgt_path=train_tgt)\n",
    "generate_new(top_k_similar=10, distance=0.5, retrieve_dataset=de_lines, query_lines=val_de_lines, retrieve_tar_sen_dataset=hsb_lines, query_tar_sen_dataset=val_hsb_lines, new_src_path=val_src, new_tgt_path=val_tgt)\n",
    "generate_new(top_k_similar=2, distance=0.5, retrieve_dataset=de_lines, query_lines=test_de_lines, retrieve_tar_sen_dataset=hsb_lines, query_tar_sen_dataset=test_hsb_lines,  new_src_path=test_src, new_tgt_path=test_tgt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60c69335-9d2a-4d24-93ef-5cb3dfea3697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# similar domain selection\n",
    "mono_de = \"./data/toy_data/news.2007.de.shuffled.deduped\"\n",
    "created_src = './data/{}/created.dehsb_hsb.dehsb'.format(experiments)\n",
    "mono_src = './data/aug_double/mono.de_hsb.de'\n",
    "mono_de_lines = read_data(mono_de)\n",
    "similar_domain_selection(2, mono_de_lines, de_lines, hsb_lines, created_src, mono_src)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cf28231-2fc1-4cb7-b05d-e8483c6e2153",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# we list the newly generated test set as an example\n",
    "new_test_de_lines = read_data(test_src)\n",
    "new_test_hsb_lines = read_data(test_tgt)\n",
    "\n",
    "for s,t in zip(new_test_de_lines[:-1],new_test_hsb_lines[:-1]):\n",
    "    print(\"source sentences: \", s, \"\\n\")\n",
    "    print(\"target sentence: \", t, \"\\n\")\n",
    "    print(\"-------------------------------------------------------------------------------------------------------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2b8d9e-ac4b-46b4-af9b-0d75c7f8a431",
   "metadata": {},
   "source": [
    "## Leveraging A GAN Generator to Implement Back Translation from the Source Side\n",
    "\n",
    "The GAN structure enables the utilization of additional source-side monolingual corpus to improve the performance of the translation model.\n",
    "\n",
    "<img src=\"./figure/GAN.png\" width=\"40%\" style=\"margin: 0 auto;\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1622d007-7a14-42d6-82ba-d605b0528ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we begin to train the generator in the architecture above.\n",
    "\n",
    "import train_GAN\n",
    "train_GAN.begin_training()\n",
    "\n",
    "# In the train log below, we first print some important parameters.\n",
    "# In our actual experiments, we leverage early stop and save the best model to ensure the performance.\n",
    "# Followed with them, is the structure of G and D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af1108d3-f5d3-4a7d-8475-68b78ce9e7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# translate and evaluate the test set\n",
    "import translate\n",
    "translate.main(test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f948adf-4453-4f07-abe4-2c9683742c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluation\n",
    "from tabulate import tabulate\n",
    "table_header = ['Generator', 'BLEU', 'chrF2', 'TER']\n",
    "bleu, chrF2, ter = evaluation.cal_bleu()\n",
    "print(tabulate(tabular_data=[('+TM +GAN', bleu, chrF2, ter)],headers=table_header,tablefmt='grid'))\n",
    "# evaluation.main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a28754-c841-420c-b8f0-f9b14d401f07",
   "metadata": {},
   "source": [
    "## Conducting Data Augmentation experiments to Evaluate the Performance of High-quality Filter.\n",
    "\n",
    "Not all sentences translated by the generator are of high quality. \n",
    "\n",
    "Therefore, we implemented the following process to filter out sentences that significantly deviate from natural language sentences. \n",
    "\n",
    "We retained high-quality translation results and demonstrated their effectiveness through data augmentation experiments.\n",
    "\n",
    "<img src=\"./figure/filter.png\" width=\"40%\" style=\"margin: 0 auto;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6a7d178-de54-49c8-bda5-96fdfc999fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# translate the monolingual corpus in similar domain\n",
    "# We use a pretrain model to translate the monolingual corpus, to show the true situation of our filter below.\n",
    "# In your case, you should use the generator (G) trained above.\n",
    "# Please modify the model_config.py file.\n",
    "import translate\n",
    "translate.main(test=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd44391a-4e92-45fc-8cc0-c5f1647ff6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp ./result/GAN_TM/translation.txt ./data/aug_double/created.de_hsb.hsb\n",
    "!cp ./data/toy_data/train.hsb-de.de ./data/aug_double/train.de_hsb.de\n",
    "!cp ./data/toy_data/train.hsb-de.hsb ./data/aug_double/train.de_hsb.hsb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "072007be-6a69-4702-95c7-bf592796cfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the filter interval in natural bilingual corpus\n",
    "import high_quality_procedure.high_quality_filter as filter\n",
    "ppl_mean, ppl_std, len_mean, len_std = filter.get_original_interval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0384248-d9ef-49e4-98e8-0b59644cdf1d",
   "metadata": {},
   "source": [
    "Function filter.filter_synthetic() below accepts four parameters to establish criteria for the filter. \n",
    "\n",
    "To conduct comparative experiments, adjust the standard intervals in filter.filter_synthetic() while keeping other procedures unchanged. \n",
    "\n",
    "For example:\n",
    "\n",
    "- filter.filter_synthetic(0, 100, len_mean+len_std, len_mean-len_std) means filter the synthetic sentences by the ratio of length.\n",
    "\n",
    "This process is straightforward and repetitive, utilizing vanilla Transformer models throughout. \n",
    "\n",
    "Therefore, further elaboration is unnecessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6951bea8-94d9-41fc-8fc3-2b7806fe020d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the synthetic bilingual corpus\n",
    "filter.filter_synthetic(ppl_mean+ppl_std, ppl_mean-ppl_std, len_mean+len_std, len_mean-len_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63d75b63-87f3-4804-8631-fcbc8f045725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# augment the original bilingual corpus.\n",
    "!cat ./data/toy_data/train.hsb-de.de ./data/aug_double/filtered.de_hsb.de > ./data/aug_double/train.de_hsb.de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f32595f-15a5-41df-ba1d-83f8eca024f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ./data/toy_data/train.hsb-de.hsb ./data/aug_double/filtered.de_hsb.hsb > ./data/aug_double/train.de_hsb.hsb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1a0c3e5-fb78-4347-b875-d8523b45ee2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data augmentation experiment.\n",
    "import transformer\n",
    "transformer.begin_training()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b393626-f459-46c5-9b93-f77c67b4f3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# translate and evaluate test set.\n",
    "from model_config import Transformer_Config\n",
    "config = Transformer_Config()\n",
    "translate.main(test=True, config=config)\n",
    "table_header = ['Filter', 'BLEU', 'chrF2', 'TER']\n",
    "bleu, chrF2, ter = evaluation.cal_bleu(config=config)\n",
    "print(tabulate(tabular_data=[('+ratio_of_ppl +ratio_of_len', bleu, chrF2, ter)],headers=table_header,tablefmt='grid'))\n",
    "# evaluation.main(config=config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
